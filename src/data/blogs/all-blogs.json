{
  "generated_at": "2026-01-30T12:00:00.000Z",
  "count": 4,
  "entries": [
    {
      "id": "v710-training-api-release",
      "title": "MLOS v7.1.0: Introducing the Training Workload API",
      "date": "2026-01-30",
      "author": {
        "name": "MLOS Core Team",
        "github": "mlOS-foundation",
        "role": "Core Development Team"
      },
      "category": "release",
      "featured": true,
      "badge": "NEW RELEASE",
      "summary": "MLOS v7.1.0 introduces the Training Workload API, bringing OS-level optimizations to ML training with 15-30% efficiency gains through NUMA-aware allocation, phase-aware scheduling, and safe checkpointing.",
      "reading_time": "8 min read",
      "content": "# MLOS v7.1.0: Training Workload API\n\nWe're excited to announce MLOS Core v7.1.0, featuring the new **Training Workload API** (`libmlos-training`). This release extends MLOS beyond inference to provide OS-level optimizations for ML training workloads.\n\n## Why Training Workload API?\n\nTraining workloads have unique characteristics that generic operating systems aren't optimized for:\n\n- **Repetitive patterns**: Forward pass → backward pass → optimizer → repeat\n- **Phase transitions**: Data loading, compute, I/O phases with different resource needs\n- **Memory pressure**: Gradient accumulation and checkpointing under memory constraints\n- **Distributed coordination**: Multi-node training requires efficient synchronization\n\nThe Training Workload API addresses these by providing kernel-level awareness of training patterns.\n\n---\n\n## Key Features\n\n### NUMA-Aware Resource Allocation\n\nAutomatically place training data and model weights on optimal NUMA nodes:\n\n```c\nmlos_training_resource_request_t request = {\n    .num_cpus = 8,\n    .memory_bytes = 16ULL * 1024 * 1024 * 1024,\n    .numa_node_preference = -1,  // Auto-select\n    .exclusive_cpus = true\n};\nmlos_training_request_resources(&request, &resources);\n```\n\nThis reduces cross-socket memory latency by up to 40%.\n\n### Training Phase Hints\n\nThe API recognizes six training phases, each with different scheduling priorities:\n\n| Phase | Priority | Characteristics |\n|-------|----------|----------------|\n| IDLE | Lowest | Workload inactive |\n| DATA_LOADING | Low | I/O bound, prefetch optimization |\n| FORWARD | Normal | Compute bound |\n| BACKWARD | High | Compute + memory, gradient pinning |\n| OPTIMIZER | Normal | Weight updates |\n| CHECKPOINT | Critical | I/O bound, memory-locked |\n\n### Safe Checkpointing\n\nMemory-locked buffers ensure checkpoint saves complete even under memory pressure:\n\n```c\nvoid* buffer = mlos_training_alloc_checkpoint_buffer(resources, size);\n// Buffer is mlock'd - won't be swapped out\nsave_checkpoint_to_buffer(buffer);\n```\n\n### Distributed Primitives\n\nOS-level barrier and all-reduce operations with lower latency than userspace implementations:\n\n```c\nmlos_training_barrier(resources, rank, world_size);\nmlos_training_allreduce(resources, gradients, count, MLOS_REDUCE_SUM);\n```\n\n---\n\n## Performance Results\n\n| Metric | Value |\n|--------|-------|\n| Training Efficiency Gain | 15-30% |\n| Concurrent Workload Overhead | <15% |\n| Unit Tests | 61 passing |\n| Integration Tests | 19 passing |\n\n---\n\n## Real-World Validation: Artifactiq YOLO Training\n\nWe validated the Training Workload API using [Artifactiq's](https://artifactiq.app) YOLO training pipeline:\n\n**Training Configuration:**\n- Model: YOLOv8n\n- Epochs: 5 (CI validation)\n- Image Size: 320x320\n- Framework: PyTorch + Ultralytics\n\n**Results:**\n- FP32 ONNX Export: 11.6 MB\n- FP16 ONNX Export: 5.8 MB\n- All exports verified successfully\n\nThe integration demonstrates end-to-end training with automatic ONNX export for deployment via MLOS Core inference.\n\n---\n\n## Getting Started\n\n### Installation\n\n```bash\ncurl -fsSL https://mlosfoundation.org/install.sh | bash\nmlos --version\n# MLOS Core v7.1.0 (Training API enabled)\n```\n\n### Basic Usage\n\n```c\n#include <mlos/training.h>\n\n// Request resources\nmlos_training_resources_t* resources;\nmlos_training_request_resources(&request, &resources);\n\n// Bind thread\nmlos_training_bind_thread(resources, pthread_self(), 0);\n\n// Training loop with phase hints\nfor (int epoch = 0; epoch < num_epochs; epoch++) {\n    mlos_training_set_phase(resources, MLOS_TRAINING_PHASE_DATA_LOADING);\n    load_batch();\n    \n    mlos_training_set_phase(resources, MLOS_TRAINING_PHASE_FORWARD);\n    forward_pass();\n    \n    mlos_training_set_phase(resources, MLOS_TRAINING_PHASE_BACKWARD);\n    backward_pass();\n    \n    mlos_training_set_phase(resources, MLOS_TRAINING_PHASE_OPTIMIZER);\n    update_weights();\n}\n\n// Cleanup\nmlos_training_release_resources(resources);\n```\n\n---\n\n## Documentation\n\n- [Training Workload API Reference](/training-workload.html)\n- [Complete Training Guide](/training-guide.html)\n- [PyTorch Integration Examples](https://github.com/mlOS-foundation/examples)\n\n---\n\n## What's Next\n\nv7.2.0 will focus on:\n- GPU memory training optimizations\n- Automatic mixed precision integration\n- Enhanced distributed training primitives\n- Profile-guided phase detection\n\n---\n\n*The MLOS Core v7.1.0 release is available now via [core-releases](https://github.com/mlOS-foundation/core-releases/releases/tag/v7.1.0).*",
      "key_highlights": [
        {
          "title": "15-30% Efficiency Gains",
          "description": "OS-level training optimizations through NUMA-aware allocation and phase scheduling",
          "icon": "trending-up"
        },
        {
          "title": "6 Training Phases",
          "description": "Phase-aware scheduling for data loading, forward, backward, optimizer, and checkpoint",
          "icon": "layers"
        },
        {
          "title": "80 Tests Passing",
          "description": "Comprehensive test coverage with 61 unit tests and 19 integration tests",
          "icon": "check-circle"
        },
        {
          "title": "Safe Checkpointing",
          "description": "Memory-locked buffers ensure checkpoint saves complete under memory pressure",
          "icon": "shield"
        }
      ],
      "links": [
        {
          "text": "Read Full Post",
          "url": "https://mlosfoundation.org/blog.html?id=v710-training-api-release",
          "primary": true
        },
        {
          "text": "Training Guide",
          "url": "https://mlosfoundation.org/training-guide.html",
          "primary": false
        },
        {
          "text": "API Reference",
          "url": "https://mlosfoundation.org/training-workload.html",
          "primary": false
        },
        {
          "text": "v7.1.0 Release",
          "url": "https://github.com/mlOS-foundation/core-releases/releases/tag/v7.1.0",
          "primary": false
        }
      ],
      "tags": [
        "release",
        "training",
        "training-api",
        "numa",
        "distributed-training",
        "pytorch",
        "yolo",
        "artifactiq"
      ],
      "related_releases": [
        "v7.1.0",
        "v7.0.0"
      ],
      "source_repo": "mlOS-foundation/core-releases"
    },
    {
      "id": "mlos-automation-architecture",
      "title": "Inside MLOS: How We Automated the Entire Release, Test, and Publish Pipeline",
      "date": "2026-01-05",
      "author": {
        "name": "MLOS Core Team",
        "github": "mlOS-foundation",
        "role": "Core Development Team"
      },
      "category": "architecture",
      "featured": true,
      "badge": "ARCHITECTURE",
      "summary": "A comprehensive look at how MLOS automates its release pipeline across private and public repositories, including kernel module builds, E2E testing, and website publishing—all triggered by a single release event.",
      "reading_time": "15 min read",
      "content": "# Inside MLOS: The Fully Automated Release Pipeline\n\n## The Challenge\n\nMLOS is a complex system spanning multiple repositories with different visibility requirements:\n\n- **Private repos** contain proprietary source code and kernel patches\n- **Public repos** distribute binaries, documentation, and test reports\n- **Kernel modules** must match Core versions exactly\n- **E2E tests** validate every release across 32+ ML models\n- **Website** must stay in sync with latest releases\n\nManaging all this manually would be error-prone and time-consuming. Instead, we built a fully automated pipeline where **a single release event triggers everything**.\n\n---\n\n## Architecture Overview\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                        MLOS AUTOMATION ARCHITECTURE                         │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│   PRIVATE REPOS                         PUBLIC REPOS                        │\n│   ────────────                          ────────────                        │\n│                                                                             │\n│   ┌─────────────┐                      ┌──────────────────┐                │\n│   │    core     │─────────────────────▶│  core-releases   │                │\n│   │  (source)   │   sync-to-public     │   (binaries)     │                │\n│   └──────┬──────┘                      └────────┬─────────┘                │\n│          │                                      │                          │\n│          │ on: release                          │                          │\n│          ▼                                      ▼                          │\n│   ┌─────────────┐                      ┌──────────────────┐                │\n│   │    axon     │                      │ mlos-linux-kernel│                │\n│   │ (ML layer)  │                      │ (kernel modules) │                │\n│   └──────┬──────┘                      └────────┬─────────┘                │\n│          │                                      │                          │\n│          │ downloads                            │ downloads                │\n│          ▼                                      ▼                          │\n│   ┌─────────────┐                      ┌──────────────────┐                │\n│   │ system-test │─────────────────────▶│  GitHub Pages    │                │\n│   │  (E2E)      │   render-report      │  (test reports)  │                │\n│   └─────────────┘                      └──────────────────┘                │\n│                                                                             │\n│                         ┌──────────────────┐                               │\n│                         │ mlosfoundation   │                               │\n│                         │     .org         │                               │\n│                         │   (website)      │                               │\n│                         └──────────────────┘                               │\n│                                  ▲                                         │\n│                                  │                                         │\n│                    news-sync, blog-sync, release-sync                      │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## The Release Flow\n\nWhen a developer creates a release in the private `core` repo, a cascade of automated workflows begins:\n\n```\n┌────────────────────────────────────────────────────────────────────────────┐\n│                           RELEASE FLOW SEQUENCE                            │\n├────────────────────────────────────────────────────────────────────────────┤\n│                                                                            │\n│  1. PUBLISH ARTIFACTS                                                      │\n│     ─────────────────                                                      │\n│     ┌──────────┐                                                           │\n│     │ Developer│                                                           │\n│     │ creates  │                                                           │\n│     │ release  │                                                           │\n│     └────┬─────┘                                                           │\n│          │                                                                 │\n│          ▼                                                                 │\n│     ┌─────────────────────────────────────────────────────────────┐       │\n│     │                  publish-artifacts.yml                       │       │\n│     │  ┌─────────────────────────────────────────────────────┐    │       │\n│     │  │ Build Matrix (4 platforms in parallel):             │    │       │\n│     │  │  • linux-amd64  (gcc)                               │    │       │\n│     │  │  • linux-arm64  (aarch64-linux-gnu-gcc)             │    │       │\n│     │  │  • darwin-amd64 (clang -arch x86_64)                │    │       │\n│     │  │  • darwin-arm64 (clang -arch arm64)                 │    │       │\n│     │  └─────────────────────────────────────────────────────┘    │       │\n│     │                          │                                   │       │\n│     │                          ▼                                   │       │\n│     │  ┌─────────────────────────────────────────────────────┐    │       │\n│     │  │ Dependencies bundled:                               │    │       │\n│     │  │  • ONNX Runtime 1.18.0                              │    │       │\n│     │  │  • llama.cpp (GGUF support)                         │    │       │\n│     │  │  • libtorch (PyTorch runtime)                       │    │       │\n│     │  └─────────────────────────────────────────────────────┘    │       │\n│     │                          │                                   │       │\n│     │                          ▼                                   │       │\n│     │  ┌─────────────────────────────────────────────────────┐    │       │\n│     │  │ Outputs:                                            │    │       │\n│     │  │  • mlos-core_{version}_{platform}.tar.gz            │    │       │\n│     │  │  • mlos-core_{version}_amd64.deb                    │    │       │\n│     │  │  • SHA256 checksums                                 │    │       │\n│     │  │  • GPG signatures (optional)                        │    │       │\n│     │  └─────────────────────────────────────────────────────┘    │       │\n│     └─────────────────────────────────────────────────────────────┘       │\n│                                                                            │\n│  2. SYNC TO PUBLIC                                                         │\n│     ───────────────                                                        │\n│          │                                                                 │\n│          │ on: release [published]                                         │\n│          ▼                                                                 │\n│     ┌─────────────────────────────────────────────────────────────┐       │\n│     │                   sync-to-public.yml                         │       │\n│     │                                                              │       │\n│     │  • Download artifacts from private release                   │       │\n│     │  • Filter: Only binaries (.tar.gz, .deb, .sha256)           │       │\n│     │  • Exclude: Source code archives                             │       │\n│     │  • Create matching release in core-releases                  │       │\n│     │  • Upload artifacts to public release                        │       │\n│     │  • Trigger kernel module build                               │       │\n│     └─────────────────────────────────────────────────────────────┘       │\n│                                                                            │\n│  3. KERNEL MODULE BUILD                                                    │\n│     ───────────────────                                                    │\n│          │                                                                 │\n│          │ triggered by sync-to-public                                     │\n│          ▼                                                                 │\n│     ┌─────────────────────────────────────────────────────────────┐       │\n│     │                sync-kernel-patches.yml                       │       │\n│     │                                                              │       │\n│     │  Build kernel modules for supported kernels:                 │       │\n│     │  ┌─────────────────────────────────────────────────────┐    │       │\n│     │  │  • mlos-ml-v{version}-linux-5.15.ko                 │    │       │\n│     │  │  • mlos-ml-v{version}-linux-6.1.ko                  │    │       │\n│     │  │  • mlos-ml-v{version}-linux-6.5.ko                  │    │       │\n│     │  │  • mlos-ml-v{version}-linux-6.8.ko                  │    │       │\n│     │  └─────────────────────────────────────────────────────┘    │       │\n│     │                                                              │       │\n│     │  Published to: mlos-linux-kernel releases                    │       │\n│     └─────────────────────────────────────────────────────────────┘       │\n│                                                                            │\n└────────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## E2E Testing Pipeline\n\nEvery release is validated against 32+ ML models in a unified testing pipeline:\n\n```\n┌────────────────────────────────────────────────────────────────────────────┐\n│                          E2E TESTING PIPELINE                              │\n├────────────────────────────────────────────────────────────────────────────┤\n│                                                                            │\n│  SYSTEM-TEST REPO (Private)                                                │\n│  ─────────────────────────                                                 │\n│                                                                            │\n│     e2e-unified.yml                                                        │\n│     ┌──────────────────────────────────────────────────────────────┐      │\n│     │                                                              │      │\n│     │  ┌────────────────────────────────────────────────────────┐ │      │\n│     │  │ PHASE 1: USERSPACE TESTS                               │ │      │\n│     │  │                                                        │ │      │\n│     │  │  • Ensure kernel module NOT loaded                     │ │      │\n│     │  │  • Start Core (userspace mode)                         │ │      │\n│     │  │  • Test 32+ models via Axon                            │ │      │\n│     │  │  • Golden image validation                             │ │      │\n│     │  │  • Record inference times                              │ │      │\n│     │  └────────────────────────────────────────────────────────┘ │      │\n│     │                          │                                   │      │\n│     │                          ▼                                   │      │\n│     │  ┌────────────────────────────────────────────────────────┐ │      │\n│     │  │ CLEANUP                                                │ │      │\n│     │  │  • Drop kernel caches                                  │ │      │\n│     │  │  • Clear model caches                                  │ │      │\n│     │  │  • Ensure fair comparison                              │ │      │\n│     │  └────────────────────────────────────────────────────────┘ │      │\n│     │                          │                                   │      │\n│     │                          ▼                                   │      │\n│     │  ┌────────────────────────────────────────────────────────┐ │      │\n│     │  │ PHASE 2: KERNEL TESTS                                  │ │      │\n│     │  │                                                        │ │      │\n│     │  │  • Download kernel module from mlos-linux-kernel       │ │      │\n│     │  │  • Load module: insmod mlos-ml.ko                      │ │      │\n│     │  │  • Start Core (kernel mode)                            │ │      │\n│     │  │  • Test same 32+ models                                │ │      │\n│     │  │  • Compare with userspace results                      │ │      │\n│     │  └────────────────────────────────────────────────────────┘ │      │\n│     │                          │                                   │      │\n│     │                          ▼                                   │      │\n│     │  ┌────────────────────────────────────────────────────────┐ │      │\n│     │  │ GENERATE COMPARISON                                    │ │      │\n│     │  │                                                        │ │      │\n│     │  │  • Calculate kernel vs userspace speedup               │ │      │\n│     │  │  • Track historical metrics                            │ │      │\n│     │  │  • Upload metrics artifact                             │ │      │\n│     │  │  • Trigger render-report.yml                           │ │      │\n│     │  └────────────────────────────────────────────────────────┘ │      │\n│     └──────────────────────────────────────────────────────────────┘      │\n│                                                                            │\n│     render-report.yml                                                      │\n│     ┌──────────────────────────────────────────────────────────────┐      │\n│     │                                                              │      │\n│     │  • Download metrics from E2E run                             │      │\n│     │  • Render HTML report with Jinja2                            │      │\n│     │  • Include kernel comparison charts                          │      │\n│     │  • Deploy to GitHub Pages                                    │      │\n│     │                                                              │      │\n│     │  Output: https://mlos-foundation.github.io/system-test/      │      │\n│     └──────────────────────────────────────────────────────────────┘      │\n│                                                                            │\n└────────────────────────────────────────────────────────────────────────────┘\n```\n\n### Model Categories Tested\n\n| Category | Models | Validation |\n|----------|--------|------------|\n| NLP | BERT, RoBERTa, DistilBERT, ALBERT, etc. | Output shape, top-k class match |\n| Embeddings | BGE, E5, GTE, MiniLM | Embedding dimensions |\n| Vision | ResNet, ViT, ConvNeXt, RegNet, BEiT | Classification accuracy |\n| Sentiment | DistilRoBERTa, SqueezeBERT, BART | Sentiment scores |\n\n---\n\n## Website Publishing Pipeline\n\nContent flows from multiple sources to the website:\n\n```\n┌────────────────────────────────────────────────────────────────────────────┐\n│                        WEBSITE PUBLISHING PIPELINE                         │\n├────────────────────────────────────────────────────────────────────────────┤\n│                                                                            │\n│  SOURCE REPOS                              WEBSITE                         │\n│  ────────────                              ───────                         │\n│                                                                            │\n│  ┌─────────────┐                                                           │\n│  │    core     │                                                           │\n│  │             │                                                           │\n│  │ docs/news/  │──┐                                                        │\n│  │ entries/    │  │                                                        │\n│  └─────────────┘  │  sync-news-to-website.yml                              │\n│                   │  ┌────────────────────────┐                            │\n│                   ├─▶│ repository_dispatch    │                            │\n│                   │  │ event: news-update     │──┐                         │\n│                   │  └────────────────────────┘  │                         │\n│                   │                              │                         │\n│  ┌─────────────┐  │                              │                         │\n│  │core-releases│  │                              │                         │\n│  │             │  │  sync-blogs-to-website.yml   │                         │\n│  │ docs/blogs/ │──┘  ┌────────────────────────┐  │                         │\n│  │ entries/    │────▶│ repository_dispatch    │──┤                         │\n│  └─────────────┘     │ event: blog-update     │  │                         │\n│                      └────────────────────────┘  │                         │\n│                                                  │                         │\n│  ┌─────────────┐     sync-release-to-website.yml │                         │\n│  │    core     │     ┌────────────────────────┐  │                         │\n│  │ on: release │────▶│ repository_dispatch    │──┤                         │\n│  │ [published] │     │ event: release-update  │  │                         │\n│  └─────────────┘     └────────────────────────┘  │                         │\n│                                                  │                         │\n│                                                  ▼                         │\n│                      ┌────────────────────────────────────────────┐       │\n│                      │           mlosfoundation.org                │       │\n│                      │                                            │       │\n│                      │  ┌──────────────────────────────────────┐  │       │\n│                      │  │ receive-news.yml                     │  │       │\n│                      │  │  • Aggregate news from all sources   │  │       │\n│                      │  │  • Update src/data/news/all-news.json│  │       │\n│                      │  │  • Commit and push to main           │  │       │\n│                      │  └──────────────────────────────────────┘  │       │\n│                      │                                            │       │\n│                      │  ┌──────────────────────────────────────┐  │       │\n│                      │  │ receive-blogs.yml                    │  │       │\n│                      │  │  • Fetch blogs from public repos     │  │       │\n│                      │  │  • Schedule: every 6 hours           │  │       │\n│                      │  │  • Update src/data/blogs/all-blogs   │  │       │\n│                      │  └──────────────────────────────────────┘  │       │\n│                      │                                            │       │\n│                      │  ┌──────────────────────────────────────┐  │       │\n│                      │  │ receive-release.yml                  │  │       │\n│                      │  │  • Update site-config.json           │  │       │\n│                      │  │  • Update version badges             │  │       │\n│                      │  │  • Update hero section               │  │       │\n│                      │  └──────────────────────────────────────┘  │       │\n│                      │                                            │       │\n│                      └────────────────────────────────────────────┘       │\n│                                                                            │\n└────────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Private vs Public Boundary\n\nThe key architectural decision is **what stays private and what becomes public**:\n\n```\n┌────────────────────────────────────────────────────────────────────────────┐\n│                         VISIBILITY BOUNDARY                                │\n├────────────────────────────────────────────────────────────────────────────┤\n│                                                                            │\n│  PRIVATE (Source Code)                PUBLIC (Binaries & Reports)          │\n│  ─────────────────────                ───────────────────────────          │\n│                                                                            │\n│  ┌────────────────────┐              ┌────────────────────────────┐       │\n│  │ mlOS-foundation/   │              │ mlOS-foundation/           │       │\n│  │ core               │              │ core-releases              │       │\n│  │                    │              │                            │       │\n│  │ • C source code    │     ═══▶     │ • Pre-built binaries       │       │\n│  │ • Makefiles        │              │ • .deb packages            │       │\n│  │ • Kernel patches   │              │ • SHA256 checksums         │       │\n│  │ • Internal docs    │              │ • Release notes            │       │\n│  │ • News entries     │              │ • Blog posts               │       │\n│  └────────────────────┘              └────────────────────────────┘       │\n│                                                                            │\n│  ┌────────────────────┐              ┌────────────────────────────┐       │\n│  │ mlOS-foundation/   │              │ mlOS-foundation/           │       │\n│  │ axon               │              │ mlos-linux-kernel          │       │\n│  │                    │              │                            │       │\n│  │ • Go source code   │     ═══▶     │ • Compiled .ko modules     │       │\n│  │ • Converter logic  │              │ • Multi-kernel support     │       │\n│  │ • Model handlers   │              │ • Version-matched          │       │\n│  └────────────────────┘              └────────────────────────────┘       │\n│                                                                            │\n│  ┌────────────────────┐              ┌────────────────────────────┐       │\n│  │ mlOS-foundation/   │              │ GitHub Pages               │       │\n│  │ system-test        │              │ mlos-foundation.github.io/ │       │\n│  │                    │              │ system-test/               │       │\n│  │ • Test scripts     │     ═══▶     │                            │       │\n│  │ • Golden images    │              │ • E2E test reports         │       │\n│  │ • Report templates │              │ • Kernel comparison        │       │\n│  │ • Model configs    │              │ • Historical metrics       │       │\n│  └────────────────────┘              └────────────────────────────┘       │\n│                                                                            │\n│  ═══▶ = sync-to-public.yml / render-report.yml / sync-blogs.yml           │\n│                                                                            │\n│  WHAT STAYS PRIVATE:                 WHAT BECOMES PUBLIC:                  │\n│  • Source code                       • Compiled binaries                   │\n│  • Build scripts                     • Test results                        │\n│  • Internal docs                     • Performance metrics                 │\n│  • Proprietary algorithms            • Installation guides                 │\n│                                      • News & blog posts                   │\n│                                                                            │\n└────────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Secrets and Cross-Repo Authentication\n\nCross-repo automation requires careful secret management:\n\n```\n┌────────────────────────────────────────────────────────────────────────────┐\n│                            SECRET MANAGEMENT                               │\n├────────────────────────────────────────────────────────────────────────────┤\n│                                                                            │\n│  REPO                  SECRET                     PURPOSE                  │\n│  ────                  ──────                     ───────                  │\n│                                                                            │\n│  core                  CORE_RELEASES_TOKEN        Push to core-releases    │\n│                        WEBSITE_DISPATCH_TOKEN     Trigger website updates  │\n│                        GPG_PRIVATE_KEY            Sign release artifacts   │\n│                        GPG_PASSPHRASE             GPG key passphrase       │\n│                                                                            │\n│  core-releases         (none required)            Public repo, no secrets  │\n│                                                                            │\n│  system-test           GITHUB_TOKEN               Download from releases   │\n│                                                   (built-in, read-only)    │\n│                                                                            │\n│  mlosfoundation.org    GITHUB_TOKEN               Auto-commit updates      │\n│                                                   (built-in, write perms)  │\n│                                                                            │\n│  ┌──────────────────────────────────────────────────────────────────────┐ │\n│  │ FALLBACK STRATEGY                                                    │ │\n│  │                                                                      │ │\n│  │ If WEBSITE_DISPATCH_TOKEN is not set:                                │ │\n│  │   • Workflows complete but skip cross-repo triggers                  │ │\n│  │   • Website polls public repos on schedule (every 6 hours)           │ │\n│  │   • No manual intervention required                                  │ │\n│  │                                                                      │ │\n│  │ Benefits:                                                            │ │\n│  │   • System works even without PAT configuration                      │ │\n│  │   • Graceful degradation, not failure                                │ │\n│  └──────────────────────────────────────────────────────────────────────┘ │\n│                                                                            │\n└────────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Complete Flow: From Commit to Live\n\nHere's what happens when we release a new version:\n\n```\n┌────────────────────────────────────────────────────────────────────────────┐\n│                     COMPLETE RELEASE TIMELINE                              │\n├────────────────────────────────────────────────────────────────────────────┤\n│                                                                            │\n│  T+0m    Developer creates release v7.0.0 in core                          │\n│          │                                                                 │\n│          ├──▶ publish-artifacts.yml triggered                              │\n│          │                                                                 │\n│  T+5m    │    Build matrix completes (4 platforms)                         │\n│          │    • linux-amd64, linux-arm64                                   │\n│          │    • darwin-amd64, darwin-arm64                                 │\n│          │                                                                 │\n│  T+8m    │    .deb package built                                           │\n│          │                                                                 │\n│  T+10m   │    Artifacts uploaded, release published                        │\n│          │                                                                 │\n│          ├──▶ sync-to-public.yml triggered (on: release published)         │\n│          │                                                                 │\n│  T+12m   │    Binaries synced to core-releases                             │\n│          │                                                                 │\n│          ├──▶ sync-kernel-patches.yml triggered                            │\n│          │                                                                 │\n│  T+20m   │    Kernel modules built for 4 kernel versions                   │\n│          │    Published to mlos-linux-kernel                               │\n│          │                                                                 │\n│          ├──▶ sync-release-to-website.yml triggered                        │\n│          │                                                                 │\n│  T+22m   │    Website updated with new version                             │\n│          │    • site-config.json updated                                   │\n│          │    • Hero badge: \"STABLE RELEASE - v7.0.0\"                      │\n│          │                                                                 │\n│          ├──▶ E2E tests manually triggered (or weekly schedule)            │\n│          │                                                                 │\n│  T+45m   │    e2e-unified.yml completes                                    │\n│          │    • Userspace: 32 models tested                                │\n│          │    • Kernel: 32 models tested                                   │\n│          │    • Comparison metrics generated                               │\n│          │                                                                 │\n│          ├──▶ render-report.yml triggered                                  │\n│          │                                                                 │\n│  T+48m   │    Test report published to GitHub Pages                        │\n│          │    https://mlos-foundation.github.io/system-test/               │\n│          │                                                                 │\n│  T+54m   │    Blog sync runs (scheduled or triggered)                      │\n│          │    New blog posts appear on mlosfoundation.org                  │\n│          │                                                                 │\n│  ═════   DONE - All systems updated automatically                          │\n│                                                                            │\n│  TOTAL TIME: ~1 hour from commit to fully deployed                         │\n│                                                                            │\n└────────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Key Design Decisions\n\n### 1. Event-Driven Architecture\n\nEverything triggers from a single event: `on: release [published]`. This ensures:\n- Artifacts are fully uploaded before sync begins\n- No partial releases reach public repos\n- Clear audit trail in GitHub Actions\n\n### 2. Graceful Degradation\n\nIf secrets aren't configured, workflows don't fail—they skip the cross-repo steps and continue. The website falls back to scheduled polling.\n\n### 3. Version Matching\n\nKernel modules are tagged with Core version (e.g., `mlos-ml-v7.0.0-linux-6.8.ko`). The E2E pipeline downloads the exact matching version.\n\n### 4. Golden Image Validation\n\nEvery model has expected outputs stored in `golden-test-data.yaml`. Tests validate:\n- Status success\n- Output shape matches\n- Top-k class predictions match\n\n### 5. Historical Tracking\n\nThe `historical-metrics.py` script maintains run history across E2E executions, enabling trend analysis and regression detection.\n\n---\n\n## Monitoring and Debugging\n\n### Workflow Status Dashboard\n\n| Repository | Workflow | Purpose | Schedule |\n|------------|----------|---------|----------|\n| core | publish-artifacts.yml | Build binaries | On release |\n| core | sync-to-public.yml | Sync to core-releases | On release |\n| core | sync-kernel-patches.yml | Build kernel modules | On release |\n| core | sync-news-to-website.yml | Sync news | On push to docs/news |\n| core-releases | sync-blogs-to-website.yml | Sync blogs | On push to docs/blogs |\n| system-test | e2e-unified.yml | Run E2E tests | Weekly + manual |\n| system-test | render-report.yml | Publish reports | After E2E |\n| mlosfoundation.org | receive-blogs.yml | Fetch blogs | Every 6 hours |\n\n### Common Issues and Fixes\n\n| Issue | Cause | Fix |\n|-------|-------|-----|\n| Sync skipped | WEBSITE_DISPATCH_TOKEN not set | Add PAT to secrets |\n| Kernel load fails | Module vermagic mismatch | Rebuild for runner kernel |\n| E2E fails | Golden data missing | Add model to golden-test-data.yaml |\n| Report empty | No metrics artifact | Check E2E run completed |\n\n---\n\n## Conclusion\n\nThis automation architecture enables MLOS to:\n\n1. **Release confidently** - Binaries are built, tested, and published automatically\n2. **Protect IP** - Source code stays private while binaries are public\n3. **Validate thoroughly** - Every release is tested against 32+ models\n4. **Communicate effectively** - Website, news, and blogs stay in sync\n5. **Scale safely** - Adding new models or platforms is straightforward\n\nThe entire system runs on GitHub Actions with no external CI/CD infrastructure, keeping operational complexity minimal while enabling a sophisticated multi-repo workflow.\n\n---\n\n*For more details, explore the workflow files in each repository:*\n- [core/.github/workflows/](https://github.com/mlOS-foundation/core/tree/main/.github/workflows)\n- [system-test/.github/workflows/](https://github.com/mlOS-foundation/system-test/tree/main/.github/workflows)\n- [mlosfoundation.org/.github/workflows/](https://github.com/mlOS-foundation/mlosfoundation.org/tree/main/.github/workflows)",
      "key_highlights": [
        {
          "title": "6 Repos Orchestrated",
          "description": "Private and public repos working together seamlessly",
          "icon": "git-branch"
        },
        {
          "title": "32+ Models Tested",
          "description": "Every release validated against comprehensive model suite",
          "icon": "check-circle"
        },
        {
          "title": "< 1 Hour",
          "description": "From commit to fully deployed across all systems",
          "icon": "clock"
        },
        {
          "title": "Zero Manual Steps",
          "description": "Fully automated from release to website update",
          "icon": "zap"
        }
      ],
      "links": [
        {
          "text": "Read Full Post",
          "url": "https://mlosfoundation.org/blog.html?id=mlos-automation-architecture",
          "primary": true
        },
        {
          "text": "E2E Test Reports",
          "url": "https://mlos-foundation.github.io/system-test/",
          "primary": false
        },
        {
          "text": "Core Releases",
          "url": "https://github.com/mlOS-foundation/core-releases/releases",
          "primary": false
        }
      ],
      "tags": [
        "automation",
        "ci-cd",
        "github-actions",
        "architecture",
        "devops",
        "release-management",
        "testing"
      ],
      "related_releases": [
        "v7.0.0",
        "v7.0.0-beta",
        "v6.3.0-alpha"
      ],
      "source_repo": "mlOS-foundation/core-releases"
    },
    {
      "id": "tensor-memory-management-kernel-innovation",
      "title": "Deep Dive: Tensor Memory Management - The Kernel Innovation Behind MLOS Performance",
      "date": "2026-01-05",
      "author": {
        "name": "MLOS Kernel Team",
        "github": "mlOS-foundation",
        "role": "Core Kernel Development"
      },
      "category": "kernel",
      "featured": true,
      "badge": "PATENT",
      "summary": "A comprehensive technical exploration of MLOS's patented Tensor Memory Management system: Tensor Fusion Detection, Cross-Model Deduplication, and Semantic Memory Tiering - the kernel-level innovations that deliver 4× inference performance improvements.",
      "tags": [
        "kernel",
        "tensor-memory",
        "patent",
        "performance",
        "optimization",
        "deduplication",
        "numa"
      ],
      "reading_time": "18 min read",
      "links": [
        {
          "text": "Read Full Post",
          "url": "/blog.html?id=tensor-memory-management-kernel-innovation",
          "primary": true
        },
        {
          "text": "Patent Documentation",
          "url": "https://github.com/mlOS-foundation/core/blob/main/docs/PATENTS.md",
          "primary": false
        }
      ],
      "content": "## Introduction: Why Kernel-Level Tensor Memory Management?\n\nTraditional ML frameworks manage tensor memory in userspace, treating the operating system as a black box. This approach has fundamental limitations:\n\n- **No cross-model visibility**: Each model manages memory independently\n- **Redundant allocations**: Identical tensors across models waste memory\n- **NUMA-unaware placement**: Tensors allocated without considering GPU topology\n- **Reactive eviction**: Memory pressure handled after problems occur\n\nMLOS takes a radically different approach: we moved tensor memory management into the Linux kernel itself. This gives us unprecedented visibility and control over ML memory patterns across all running models.\n\nThis blog explores three patented innovations in MLOS's Tensor Memory Manager (TMM), covered under **Patent US-63/865,176: Kernel-Level Optimizations for Machine Learning Workloads**.\n\n---\n\n## Patent Overview: US-63/865,176\n\n**Title:** Kernel-Level Optimizations for Machine Learning Workloads in a Purpose-Built Operating System\n\n**Key Claims:**\n1. ML-Aware Kernel Scheduler\n2. **Tensor Memory Management** ← Focus of this blog\n3. GPU Resource Orchestration\n4. Model Context Switching\n\nThe Tensor Memory Management innovation encompasses three distinct technologies:\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                    MLOS Tensor Memory Manager (TMM)                         │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  ┌─────────────────┐  ┌─────────────────┐  ┌──────────────────────┐        │\n│  │  TENSOR FUSION  │  │  CROSS-MODEL    │  │  SEMANTIC MEMORY     │        │\n│  │   DETECTION     │  │  DEDUPLICATION  │  │      TIERING         │        │\n│  ├─────────────────┤  ├─────────────────┤  ├──────────────────────┤        │\n│  │ Sequential      │  │ SHA-256 hash    │  │ NUMA-aware           │        │\n│  │ access pattern  │  │ content         │  │ GPU-local            │        │\n│  │ analysis        │  │ detection       │  │ allocation           │        │\n│  │                 │  │                 │  │                      │        │\n│  │ Memory reuse    │  │ Copy-on-Write   │  │ Cross-node access    │        │\n│  │ for non-        │  │ semantics for   │  │ tracking for         │        │\n│  │ overlapping     │  │ safe sharing    │  │ migration decisions  │        │\n│  │ tensors         │  │                 │  │                      │        │\n│  └─────────────────┘  └─────────────────┘  └──────────────────────┘        │\n│                                                                             │\n│  Result: 25-40% memory savings across multi-model deployments              │\n└─────────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Innovation 1: Tensor Fusion Detection\n\n### The Problem: Wasteful Sequential Allocation\n\nDuring ML inference, models create many intermediate tensors. In a typical transformer forward pass:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    Traditional Tensor Lifecycle                          │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│  Time ─────────────────────────────────────────────────────────────────► │\n│                                                                          │\n│  Tensor A: █████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░  (Attention Q)         │\n│  Tensor B: ░░░░░░░░░█████████░░░░░░░░░░░░░░░░░░░  (Attention K)         │\n│  Tensor C: ░░░░░░░░░░░░░░░░░░█████████░░░░░░░░░░  (Attention V)         │\n│  Tensor D: ░░░░░░░░░░░░░░░░░░░░░░░░░░░█████████░  (FFN intermediate)    │\n│                                                                          │\n│  Memory:   ████████████████████████████████████  (All allocated)        │\n│                                                                          │\n│  Total Memory Used: A + B + C + D = 4 × size                            │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\nThese tensors have **non-overlapping lifetimes**—they're never accessed simultaneously. Traditional systems allocate separate memory for each, wasting precious GPU memory.\n\n### The MLOS Solution: Access Pattern Analysis\n\nMLOS's Tensor Fusion Detection analyzes access patterns at the kernel level:\n\n```c\n/**\n * tensors_can_fuse - Check if two tensors can share memory\n *\n * Criteria:\n * 1. Neither is pinned (can be relocated)\n * 2. Same GPU device\n * 3. Same NUMA node\n * 4. Non-overlapping access patterns (sequential)\n */\nstatic bool tensors_can_fuse(struct mlos_tensor *t1, struct mlos_tensor *t2)\n{\n    /* Can't fuse pinned tensors */\n    if (t1->state == MLOS_TENSOR_PINNED || t2->state == MLOS_TENSOR_PINNED)\n        return false;\n\n    /* Must be on same GPU */\n    if (t1->gpu != t2->gpu)\n        return false;\n\n    /* Must be on same NUMA node */\n    if (t1->numa_node != t2->numa_node)\n        return false;\n\n    /* Check for non-overlapping access (sequential) */\n    if (t1->last_access_ns < t2->last_access_ns &&\n        t1->access_count > 0 && t2->access_count > 0) {\n        return true;  /* t1 completed before t2 started */\n    }\n\n    return false;\n}\n```\n\n### Fusion Groups: Memory Sharing in Action\n\nWhen fusion candidates are identified, they're grouped together:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    MLOS Tensor Fusion Result                             │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│  Time ─────────────────────────────────────────────────────────────────► │\n│                                                                          │\n│  ┌────────────────── Fused Tensor Block ──────────────────┐              │\n│  │                                                        │              │\n│  │  Tensor A: █████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░      │              │\n│  │  Tensor B: ░░░░░░░░░█████████░░░░░░░░░░░░░░░░░░░      │              │\n│  │  Tensor C: ░░░░░░░░░░░░░░░░░░█████████░░░░░░░░░░      │              │\n│  │  Tensor D: ░░░░░░░░░░░░░░░░░░░░░░░░░░░█████████░      │              │\n│  │                                                        │              │\n│  │            ▼ All share same memory region ▼            │              │\n│  │                                                        │              │\n│  └────────────────────────────────────────────────────────┘              │\n│                                                                          │\n│  Memory:   ████████████░░░░░░░░░░░░░░░░░░░░░░░░░  (max(A,B,C,D))        │\n│                                                                          │\n│  Savings: A + B + C + D - max(A,B,C,D) = 3 × size (75% reduction!)      │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n### The Fusion Algorithm\n\n```c\nint mlos_fusion_analyze(struct mlos_model *model, u32 flags,\n                        struct mlos_fusion_group *groups, u32 max_groups,\n                        u32 *num_groups, u64 *total_savings)\n{\n    struct mlos_tensor *tensor, *other;\n    struct mlos_fusion_group *group;\n    u32 group_count = 0;\n    u64 savings = 0;\n\n    /* Iterate through model's tensors */\n    list_for_each_entry(tensor, &model->tensors, model_node) {\n        if (group_count >= max_groups)\n            break;\n\n        /* Start a new potential fusion group */\n        group = &groups[group_count];\n        group->tensor_ids[0] = tensor->id;\n        group->tensor_count = 1;\n        group->combined_size = tensor->size;\n\n        /* Track max tensor size for savings calculation */\n        size_t max_size = tensor->size;\n\n        /* Find other tensors that can fuse with this one */\n        list_for_each_entry(other, &model->tensors, model_node) {\n            if (other == tensor)\n                continue;\n\n            if (tensors_can_fuse(tensor, other)) {\n                group->tensor_ids[group->tensor_count++] = other->id;\n                group->combined_size += other->size;\n                if (other->size > max_size)\n                    max_size = other->size;\n            }\n        }\n\n        /* Calculate savings: total - max(individual sizes) */\n        if (group->tensor_count > 1) {\n            group->saved_size = group->combined_size - max_size;\n            savings += group->saved_size;\n            group_count++;\n        }\n    }\n\n    *num_groups = group_count;\n    *total_savings = savings;\n    return 0;\n}\n```\n\n### Key Innovation: Kernel-Level Visibility\n\nUnlike userspace solutions, MLOS can:\n\n1. **Track access patterns across all models** - not just within one framework\n2. **Use nanosecond-precision timestamps** - `ktime_get_ns()` provides accurate ordering\n3. **Execute fusion atomically** - kernel spinlocks ensure consistency\n4. **Coordinate with memory pressure** - integrate with LRU eviction\n\n---\n\n## Innovation 2: Cross-Model Tensor Deduplication\n\n### The Problem: Duplicate Content Across Models\n\nIn production ML deployments, multiple models often share common components:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    Common Model Components                               │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│  Model A: BERT-base              Model B: BERT-large (fine-tuned)       │\n│  ┌─────────────────────┐         ┌─────────────────────┐                │\n│  │ Embedding Layer     │    =    │ Embedding Layer     │  ← IDENTICAL  │\n│  │ (128MB)             │         │ (128MB)             │                │\n│  ├─────────────────────┤         ├─────────────────────┤                │\n│  │ Attention Weights   │   ≈     │ Attention Weights   │  ← Similar    │\n│  │ (256MB)             │         │ (512MB)             │                │\n│  ├─────────────────────┤         ├─────────────────────┤                │\n│  │ Output Layer        │    ≠    │ Output Layer        │  ← Different  │\n│  │ (32MB)              │         │ (64MB)              │                │\n│  └─────────────────────┘         └─────────────────────┘                │\n│                                                                          │\n│  Traditional: 416MB + 704MB = 1,120MB                                   │\n│  With Dedup: 416MB + 576MB = 992MB (128MB saved from shared embedding)  │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n### The MLOS Solution: Content-Based Deduplication\n\nMLOS implements a SHA-256 hash tree for O(log n) duplicate detection:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    Deduplication Hash Tree                               │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│                         ┌──────────────┐                                │\n│                         │  Root Node   │                                │\n│                         └──────┬───────┘                                │\n│                    ┌───────────┴───────────┐                            │\n│               ┌────┴────┐             ┌────┴────┐                       │\n│               │ hash<M  │             │ hash>M  │                       │\n│               └────┬────┘             └────┬────┘                       │\n│          ┌─────────┴─────────┐      ┌─────┴─────┐                       │\n│     ┌────┴────┐         ┌────┴────┐ │           │                       │\n│     │sha256_A │         │sha256_B │ │sha256_C   │                       │\n│     │tensor_1 │         │tensor_5 │ │tensor_3   │                       │\n│     │ref_cnt=2│         │ref_cnt=1│ │ref_cnt=3  │                       │\n│     └─────────┘         └─────────┘ └───────────┘                       │\n│         ↑                                   ↑                           │\n│         │                                   │                           │\n│    tensor_7 shares                    tensor_9, tensor_12               │\n│    content with tensor_1              share content with tensor_3       │\n│                                                                          │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n### SHA-256 Content Hashing\n\nMLOS uses the Linux kernel's crypto API for efficient hashing:\n\n```c\n/**\n * mlos_dedup_hash_tensor - Compute SHA-256 hash of tensor content\n * @tensor: Tensor to hash\n * @hash_out: Output buffer for 32-byte hash\n *\n * Uses kernel crypto API for hardware-accelerated hashing.\n */\nint mlos_dedup_hash_tensor(struct mlos_tensor *tensor, u8 *hash_out)\n{\n    struct crypto_shash *tfm;\n    struct shash_desc *desc;\n    int ret;\n\n    /* Allocate SHA-256 transform - may use hardware acceleration */\n    tfm = crypto_alloc_shash(\"sha256\", 0, 0);\n    if (IS_ERR(tfm))\n        return PTR_ERR(tfm);\n\n    desc = kmalloc(sizeof(*desc) + crypto_shash_descsize(tfm), GFP_KERNEL);\n    desc->tfm = tfm;\n\n    /* Hash entire tensor content */\n    ret = crypto_shash_digest(desc, tensor->cpu_addr, tensor->size, hash_out);\n\n    kfree(desc);\n    crypto_free_shash(tfm);\n\n    if (ret == 0)\n        atomic64_inc(&mlos_dedup_ctx.tensors_hashed);\n\n    return ret;\n}\n```\n\n### Copy-on-Write (CoW) Semantics\n\nDeduplicated tensors use CoW for safe sharing:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    Copy-on-Write Flow                                    │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│  Initial State (after dedup):                                           │\n│  ┌─────────────────────────────────────────────────────────────┐        │\n│  │                    Shared Memory Region                      │        │\n│  │                    [Embedding Weights]                       │        │\n│  │                                                              │        │\n│  └─────────────────────────────────────────────────────────────┘        │\n│        ↑                                           ↑                     │\n│        │ read-only                                 │ read-only           │\n│  ┌─────┴─────┐                              ┌─────┴─────┐               │\n│  │  Model A  │                              │  Model B  │               │\n│  │ (ref=1)   │                              │ (ref=1)   │               │\n│  └───────────┘                              └───────────┘               │\n│                                                                          │\n│  After Model B writes (fine-tuning):                                    │\n│  ┌──────────────────────────────┐   ┌──────────────────────────┐        │\n│  │     Original Memory          │   │     New Private Copy     │        │\n│  │    [Embedding Weights]       │   │  [Modified Embeddings]   │        │\n│  └──────────────────────────────┘   └──────────────────────────┘        │\n│        ↑                                           ↑                     │\n│        │ read-only                                 │ read-write          │\n│  ┌─────┴─────┐                              ┌─────┴─────┐               │\n│  │  Model A  │                              │  Model B  │               │\n│  │ (ref=1)   │                              │ (new ref) │               │\n│  └───────────┘                              └───────────┘               │\n│                                                                          │\n│  CoW triggered: Model B gets private copy, original unchanged           │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n### Deduplication Entry Structure\n\n```c\n/**\n * struct mlos_dedup_entry - Hash table entry for deduplication\n * @hash: Content hash (SHA-256, 32 bytes)\n * @tensor: Original tensor holding the data\n * @ref_count: Number of tensors sharing this content\n * @rb_node: Red-black tree node for O(log n) lookup\n * @flags: Dedup flags (COW_ENABLED, etc.)\n */\nstruct mlos_dedup_entry {\n    u8 hash[32];\n    struct mlos_tensor *tensor;\n    atomic_t ref_count;\n    struct rb_node rb_node;\n    u32 flags;\n};\n```\n\n### Statistics Tracking\n\nMLOS tracks comprehensive deduplication metrics:\n\n```c\nstruct mlos_dedup_context {\n    struct rb_root hash_tree;           /* O(log n) hash lookup */\n    atomic64_t tensors_hashed;          /* Total tensors analyzed */\n    atomic64_t duplicates_found;        /* Duplicates detected */\n    atomic64_t memory_saved;            /* Total bytes saved */\n    atomic64_t shared_tensors;          /* Currently shared count */\n    atomic64_t hash_collisions;         /* For robustness monitoring */\n    atomic64_t cow_triggers;            /* CoW operations performed */\n    spinlock_t lock;                    /* Thread-safe operations */\n};\n```\n\n---\n\n## Innovation 3: Semantic Memory Tiering (NUMA-Aware Allocation)\n\n### The Problem: NUMA Topology Ignorance\n\nModern servers have Non-Uniform Memory Access (NUMA) topology:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    Typical Multi-Socket Server                           │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│   NUMA Node 0                              NUMA Node 1                   │\n│  ┌────────────────────────┐              ┌────────────────────────┐     │\n│  │  CPU Socket 0          │              │  CPU Socket 1          │     │\n│  │  ┌──────────────────┐  │              │  ┌──────────────────┐  │     │\n│  │  │  Memory: 256GB   │  │              │  │  Memory: 256GB   │  │     │\n│  │  │  (LOCAL ACCESS)  │  │──────────────│  │  (LOCAL ACCESS)  │  │     │\n│  │  │    ~80ns         │  │   QPI/UPI    │  │    ~80ns         │  │     │\n│  │  └──────────────────┘  │   ~150ns     │  └──────────────────┘  │     │\n│  │                        │   (REMOTE)   │                        │     │\n│  │  PCIe ───────┐         │              │         ┌─── PCIe     │     │\n│  │              │         │              │         │              │     │\n│  │        ┌─────┴─────┐   │              │   ┌─────┴─────┐        │     │\n│  │        │  GPU 0    │   │              │   │  GPU 1    │        │     │\n│  │        │  16GB     │   │              │   │  16GB     │        │     │\n│  │        └───────────┘   │              │   └───────────┘        │     │\n│  └────────────────────────┘              └────────────────────────┘     │\n│                                                                          │\n│  Problem: Tensor for GPU 0 allocated on Node 1 = 2× latency penalty    │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n### The MLOS Solution: GPU-Aware NUMA Placement\n\nMLOS automatically places tensors on the NUMA node closest to the target GPU:\n\n```c\n/**\n * mlos_resolve_numa_node - Resolve NUMA node for allocation\n * @gpu: GPU device (used to find local NUMA node)\n * @requested_node: Requested NUMA node (-1 = any, -2 = local)\n *\n * NUMA-aware allocation per patent: Semantic Memory Tiering.\n * Places tensors on NUMA nodes closest to the executing GPU.\n */\nstatic int mlos_resolve_numa_node(struct mlos_gpu_device *gpu, int requested_node)\n{\n    int node;\n\n    if (requested_node == MLOS_NUMA_NODE_ANY) {\n        /* No preference - use current CPU's node */\n        return numa_node_id();\n    }\n\n    if (requested_node == MLOS_NUMA_NODE_LOCAL) {\n        /* Use GPU's local NUMA node if available */\n        if (gpu && gpu->pci_dev) {\n            node = dev_to_node(&gpu->pci_dev->dev);\n            if (node != NUMA_NO_NODE)\n                return node;\n        }\n        /* Fallback to current CPU's node */\n        return numa_node_id();\n    }\n\n    /* Explicit node requested - validate and use */\n    if (requested_node >= 0 && node_online(requested_node)) {\n        return requested_node;\n    }\n\n    return numa_node_id();\n}\n```\n\n### Cross-NUMA Access Tracking\n\nMLOS tracks when tensors are accessed from non-local NUMA nodes:\n\n```c\n/**\n * mlos_tensor_record_access - Record tensor access for NUMA statistics\n * @tensor: Tensor being accessed\n * @accessing_numa_node: NUMA node of the accessing CPU\n *\n * Tracks cross-NUMA node accesses for performance analysis.\n */\nvoid mlos_tensor_record_access(struct mlos_tensor *tensor, int accessing_numa_node)\n{\n    struct mlos_memory_pool *pool = &tensor->gpu->pool;\n    int tensor_node = tensor->numa_node;\n\n    spin_lock_irqsave(&pool->lock, flags);\n\n    /* Record cross-NUMA access */\n    if (tensor_node != accessing_numa_node) {\n        pool->cross_numa_accesses++;\n        pool->numa_stats[tensor_node].remote_accesses++;\n    }\n\n    spin_unlock_irqrestore(&pool->lock, flags);\n}\n```\n\n### Per-Node Statistics\n\n```c\nstruct mlos_numa_pool_stats {\n    int node_id;              /* NUMA node identifier */\n    size_t used_size;         /* Bytes allocated on this node */\n    u32 tensor_count;         /* Number of tensors on this node */\n    u64 allocations;          /* Total allocations */\n    u64 remote_accesses;      /* Accesses from other NUMA nodes */\n    u64 migrations_in;        /* Tensors migrated TO this node */\n    u64 migrations_out;       /* Tensors migrated FROM this node */\n};\n```\n\n### Migration Opportunities\n\nWhen a tensor has high remote access count, MLOS can migrate it:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    NUMA Migration Decision                               │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│  Tensor X Statistics:                                                    │\n│  ┌────────────────────────────────────────────────────────────┐         │\n│  │  Current NUMA Node: 0                                      │         │\n│  │  Local Accesses: 1,234                                     │         │\n│  │  Remote Accesses: 45,678  ← Anomaly detected!             │         │\n│  │  Remote Access Ratio: 97.4%                                │         │\n│  └────────────────────────────────────────────────────────────┘         │\n│                                                                          │\n│  Decision: Migrate tensor from Node 0 → Node 1                          │\n│                                                                          │\n│  Before:                              After:                             │\n│  ┌───────────┐    ┌───────────┐      ┌───────────┐    ┌───────────┐    │\n│  │  Node 0   │◄───│  Node 1   │      │  Node 0   │    │  Node 1   │    │\n│  │ [Tensor X]│ QPI│  [GPU 1]  │      │           │    │[Tensor X] │    │\n│  └───────────┘    └───────────┘      └───────────┘    └───────────┘    │\n│     150ns latency                        80ns latency (47% faster!)     │\n│                                                                          │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## The Complete TMM Architecture\n\nAll three innovations work together in the Tensor Memory Manager:\n\n```\n┌──────────────────────────────────────────────────────────────────────────────────┐\n│                         MLOS Tensor Memory Manager                               │\n│                              Architecture                                        │\n├──────────────────────────────────────────────────────────────────────────────────┤\n│                                                                                  │\n│  ┌─────────────────────────────────────────────────────────────────────────────┐ │\n│  │                        Userspace Interface                                  │ │\n│  │  /dev/mlos character device with ioctl + mmap                              │ │\n│  └────────────────────────────────────┬────────────────────────────────────────┘ │\n│                                       │                                          │\n│                                       ▼                                          │\n│  ┌─────────────────────────────────────────────────────────────────────────────┐ │\n│  │                         Kernel TMM Core                                     │ │\n│  │                                                                             │ │\n│  │   ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐         │ │\n│  │   │  Memory Pool    │   │  Red-Black Tree │   │  LRU List       │         │ │\n│  │   │  Management     │   │  (O(log n))     │   │  (Eviction)     │         │ │\n│  │   │                 │   │                 │   │                 │         │ │\n│  │   │  total_size     │   │  tensor_tree    │   │  ─────────────► │         │ │\n│  │   │  used_size      │   │  hash_tree      │   │  newest  oldest │         │ │\n│  │   │  fragmented     │   │                 │   │                 │         │ │\n│  │   └─────────────────┘   └─────────────────┘   └─────────────────┘         │ │\n│  │                                                                             │ │\n│  └────────────────────────────────────┬────────────────────────────────────────┘ │\n│                                       │                                          │\n│          ┌────────────────────────────┼────────────────────────────┐            │\n│          │                            │                            │            │\n│          ▼                            ▼                            ▼            │\n│  ┌───────────────────┐   ┌───────────────────┐   ┌───────────────────┐         │\n│  │  Fusion Engine    │   │  Dedup Engine     │   │  NUMA Engine      │         │\n│  ├───────────────────┤   ├───────────────────┤   ├───────────────────┤         │\n│  │ mlos_fusion_      │   │ mlos_dedup_       │   │ mlos_resolve_     │         │\n│  │   analyze()       │   │   hash_tensor()   │   │   numa_node()     │         │\n│  │                   │   │                   │   │                   │         │\n│  │ mlos_fusion_      │   │ mlos_dedup_       │   │ mlos_tensor_      │         │\n│  │   execute()       │   │   enable()        │   │   record_access() │         │\n│  │                   │   │                   │   │                   │         │\n│  │ Access pattern    │   │ SHA-256 hash tree │   │ Cross-node        │         │\n│  │ analysis          │   │ + CoW semantics   │   │ tracking          │         │\n│  └───────────────────┘   └───────────────────┘   └───────────────────┘         │\n│          │                            │                            │            │\n│          └────────────────────────────┼────────────────────────────┘            │\n│                                       │                                          │\n│                                       ▼                                          │\n│  ┌─────────────────────────────────────────────────────────────────────────────┐ │\n│  │                      Hardware Abstraction Layer                             │ │\n│  │                                                                             │ │\n│  │   ┌───────────────┐   ┌───────────────┐   ┌───────────────┐                │ │\n│  │   │  NVIDIA GPU   │   │   AMD ROCm    │   │  Intel oneAPI │                │ │\n│  │   │  via PCI      │   │               │   │               │                │ │\n│  │   └───────────────┘   └───────────────┘   └───────────────┘                │ │\n│  │                                                                             │ │\n│  └─────────────────────────────────────────────────────────────────────────────┘ │\n│                                                                                  │\n└──────────────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Performance Results\n\nOur v7.0.0 benchmarks demonstrate the combined impact:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    Memory Optimization Results                           │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│  Metric                          Before TMM    After TMM    Improvement │\n│  ────────────────────────────────────────────────────────────────────── │\n│  Memory per model (avg)          2.4 GB        1.8 GB       25% ↓       │\n│  Multi-model memory overhead     +45%          +12%         73% ↓       │\n│  Cross-NUMA accesses             38%           8%           79% ↓       │\n│  Inference latency (P99)         45ms          32ms         29% ↓       │\n│  Max concurrent models (16GB)    6             9            50% ↑       │\n│                                                                          │\n│  Tensor Fusion:                                                         │\n│  - Fusion candidates detected: 847 groups                               │\n│  - Memory saved: 1.2 GB (across 12 models)                             │\n│                                                                          │\n│  Deduplication:                                                         │\n│  - Tensors hashed: 15,234                                               │\n│  - Duplicates found: 2,156 (14.2% dedup ratio)                         │\n│  - Memory saved: 892 MB                                                 │\n│                                                                          │\n│  NUMA Optimization:                                                     │\n│  - Local allocations: 94%                                               │\n│  - Migrations performed: 127                                            │\n│  - Remote access reduction: 79%                                         │\n│                                                                          │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Why Kernel-Level Implementation?\n\nThese innovations are only possible at the kernel level:\n\n| Capability | Userspace | Kernel |\n|------------|-----------|--------|\n| Cross-model visibility | ❌ Framework silos | ✅ Global view |\n| Nanosecond timestamps | ❌ Clock overhead | ✅ ktime_get_ns() |\n| Hardware NUMA info | ❌ Indirect access | ✅ dev_to_node() |\n| Zero-copy sharing | ❌ IPC overhead | ✅ Page table sharing |\n| Atomic operations | ❌ Mutex contention | ✅ Spinlocks + atomics |\n| DMA coordination | ❌ Driver roundtrips | ✅ Direct dma_alloc |\n| LRU across models | ❌ Per-process view | ✅ System-wide LRU |\n\n---\n\n## Conclusion\n\nMLOS's Tensor Memory Management represents a fundamental rethinking of how ML systems manage memory. By moving tensor management into the kernel, we achieve:\n\n1. **Tensor Fusion**: 75% memory reduction for sequential tensors\n2. **Cross-Model Dedup**: 14%+ deduplication across model fleet\n3. **NUMA Tiering**: 79% reduction in cross-node memory accesses\n\nThese innovations are protected under **Patent US-63/865,176** and form the foundation of MLOS's performance advantage.\n\n---\n\n## Further Reading\n\n- [MLOS Kernel Architecture Overview](/blog.html?id=mlos-automation-architecture)\n- [Performance Evolution: v6.3.0 to v7.0.0](/blog.html?id=kernel-performance-evolution-v7)\n- [Patent Documentation](https://github.com/mlOS-foundation/patent-docs) (Private)\n\n---\n\n*The MLOS kernel code is available in the [core repository](https://github.com/mlOS-foundation/core) (private) with public releases distributed via [core-releases](https://github.com/mlOS-foundation/core-releases).*",
      "source_repo": "mlOS-foundation/core-releases"
    },
    {
      "id": "kernel-performance-evolution-v7",
      "title": "MLOS Core Kernel Performance Evolution: A Deep Dive into v6.3.0 to v7.0.0",
      "date": "2026-01-04",
      "author": {
        "name": "MLOS Core Team",
        "github": "mlOS-foundation",
        "role": "Core Development Team"
      },
      "category": "release-analysis",
      "featured": true,
      "badge": "DEEP DIVE",
      "summary": "A comprehensive analysis of MLOS Core's performance journey from v6.3.0-alpha through v7.0.0-beta to the stable v7.0.0 release. This post examines the kernel-level optimizations that achieved up to 40% memory reduction and 4x throughput improvements.",
      "reading_time": "12 min read",
      "content": "# MLOS Core Kernel Performance Evolution: v6.3.0 to v7.0.0\n\n## Executive Summary\n\nOver the past month, the MLOS Core kernel has undergone a significant transformation. Starting from v6.3.0-alpha's foundational improvements, through v7.0.0-beta's runtime optimizations, to the stable v7.0.0 release's memory management breakthroughs—we've achieved remarkable performance gains across the board.\n\n**Key Achievements:**\n- **25-40% memory reduction** through tensor fusion and deduplication\n- **4× reduction** in lock contention for batch inference\n- **3.6M ops/sec** tensor pool throughput\n- **<50μs** per-inference overhead (down from ~200μs)\n- **32 models** validated with comprehensive golden test data\n\n---\n\n## Release Timeline\n\n| Version | Date | Type | Focus Area |\n|---------|------|------|------------|\n| v6.3.0-alpha | Dec 31, 2025 | Alpha | Foundation & ONNX Optimization |\n| v7.0.0-beta | Jan 2, 2026 | Beta | Runtime Performance |\n| v7.0.0 | Jan 3, 2026 | Stable | Memory Optimization |\n\n---\n\n## Phase 1: Foundation (v6.3.0-alpha)\n\n### What We Started With\n\nThe v6.3.0-alpha release established the baseline for our optimization journey:\n\n- **ONNX CPU Optimizations**: Initial CPU execution path improvements\n- **Input Auto-generation**: Automatic BERT input tensor generation\n- **E2E Validation**: 32/32 models passing end-to-end tests\n- **Binary Distribution**: Cross-platform builds (Linux, macOS Intel/ARM)\n\n### Baseline Performance Metrics\n\n```\nPer-inference overhead: ~200μs\nLock acquisitions (batch=4): 4 locks\nMemory allocation: Standard malloc/free\nNUMA awareness: None\n```\n\nThis release gave us a stable foundation but highlighted several optimization opportunities in the scheduler and memory subsystems.\n\n---\n\n## Phase 2: Runtime Performance (v7.0.0-beta)\n\n### Tensor Pool Pre-allocation\n\nThe biggest runtime improvement came from eliminating per-inference allocation overhead:\n\n**Before (v6.3.0-alpha):**\n```c\n// Each inference: malloc → compute → free\nvoid* tensor = malloc(size);  // ~50-200μs\ncompute(tensor);\nfree(tensor);                 // ~10-50μs\n```\n\n**After (v7.0.0-beta):**\n```c\n// O(1) pool operations\nmlos_tensor_t* tensor = mlos_tensor_pool_acquire(pool);  // <1μs\ncompute(tensor);\nmlos_tensor_pool_release(pool, tensor);                   // <1μs\n```\n\n**Results:**\n- Throughput: **3.6 million ops/sec**\n- Overhead: Reduced from ~200μs to **<50μs** (4× improvement)\n- Lock-free fast path for common operations\n\n### Batch Inference Scheduling\n\nFor multi-task inference, we introduced batch submission:\n\n**Before:**\n```\nTask 1: acquire_lock → submit → release_lock\nTask 2: acquire_lock → submit → release_lock\nTask 3: acquire_lock → submit → release_lock\nTask 4: acquire_lock → submit → release_lock\n= 4 lock acquisitions\n```\n\n**After:**\n```\nBatch: acquire_lock → submit_all(4) → release_lock\n= 1 lock acquisition\n```\n\nThis provides **N× reduction** in lock contention for batch size N.\n\n### NUMA Thread Affinity\n\nFor multi-socket systems, we added automatic NUMA awareness:\n\n```c\nmlos_tensor_alloc_numa(size, numa_node);\nmlos_set_thread_affinity(node);\n```\n\nThis ensures inference threads run on the same NUMA node as their tensor memory, reducing cross-socket memory access latency.\n\n---\n\n## Phase 3: Memory Optimization (v7.0.0)\n\n### Tensor Fusion Detection\n\nThe stable release introduced intelligent memory reuse based on tensor lifetime analysis:\n\n**Problem:** Many tensors have non-overlapping lifetimes but are allocated separately.\n\n**Solution:** Analyze computation graph to identify tensors that can share memory:\n\n```c\nmlos_fusion_ctx_t* ctx = mlos_fusion_analyze(model);\nmlos_fusion_execute(ctx);  // Apply memory sharing\nmlos_fusion_stats_t stats = mlos_fusion_get_stats(ctx);\n// stats.memory_saved: 15-25%\n```\n\n**Results:**\n- Memory reduction: **15-25%**\n- Zero runtime overhead (analysis done at load time)\n- Automatic fallback for incompatible tensors\n\n### Cross-Model Tensor Deduplication\n\nFor multi-model deployments, we detect duplicate tensors across models:\n\n**How it works:**\n1. Compute SHA-256 hash of tensor contents\n2. Store in content-addressable cache\n3. Return existing tensor if hash matches\n4. Copy-on-write semantics for thread safety\n\n```c\nmlos_dedup_enable(global_context);\nmlos_dedup_hash_tensor(tensor);  // SHA-256 fingerprint\nmlos_dedup_analyze_models(models, count);\nmlos_dedup_stats_t stats = mlos_dedup_get_stats();\n// stats.duplicates_found, stats.memory_saved\n```\n\n**Results:**\n- Additional memory savings: **10-20%**\n- Most effective with embedding models sharing vocabulary\n- Transparent to inference code\n\n### Combined Impact\n\nWhen both optimizations are enabled:\n\n| Scenario | Fusion Only | Dedup Only | Combined |\n|----------|-------------|------------|----------|\n| Single model | 15-25% | 0% | 15-25% |\n| 2 similar models | 15-25% | 10-15% | 25-35% |\n| 4+ embedding models | 15-25% | 15-20% | 30-40% |\n\n---\n\n## Performance Comparison Summary\n\n### Latency Improvements\n\n| Metric | v6.3.0-alpha | v7.0.0-beta | v7.0.0 |\n|--------|--------------|-------------|--------|\n| Per-inference overhead | ~200μs | <50μs | <50μs |\n| Tensor acquire/release | N/A | O(1) | O(1) |\n| Lock acquisitions (batch=4) | 4 | 1 | 1 |\n| p99 latency (small models) | 2.5ms | 1.2ms | 0.8ms |\n\n### Throughput Improvements\n\n| Metric | v6.3.0-alpha | v7.0.0-beta | v7.0.0 |\n|--------|--------------|-------------|--------|\n| Tensor pool ops/sec | N/A | 3.6M | 3.6M |\n| Small model speedup | baseline | +10% | +10% |\n| Large model speedup | baseline | +40% | +40% |\n\n### Memory Improvements\n\n| Metric | v6.3.0-alpha | v7.0.0-beta | v7.0.0 |\n|--------|--------------|-------------|--------|\n| Tensor fusion savings | 0% | 0% | 15-25% |\n| Deduplication savings | 0% | 0% | 10-20% |\n| Combined savings | 0% | 0% | 25-40% |\n\n---\n\n## Test Coverage Evolution\n\n| Version | Kernel Tests | E2E Models | Golden Data |\n|---------|--------------|------------|-------------|\n| v6.3.0-alpha | 8 | 32 | 18 |\n| v7.0.0-beta | 12 | 32 | 18 |\n| v7.0.0 | 18 | 32 | 32 |\n\n**New tests in v7.0.0:**\n- Tensor fusion lifecycle and statistics\n- Cross-model deduplication with SHA-256\n- Memory pressure scenarios\n- Multi-model deployment simulation\n\n---\n\n## API Evolution\n\n### New APIs in v7.0.0-beta\n\n```c\n// Tensor Pool\nmlos_tensor_pool_create()\nmlos_tensor_pool_destroy()\nmlos_tensor_pool_acquire()\nmlos_tensor_pool_release()\nmlos_tensor_pool_get_stats()\nmlos_tensor_pool_prewarm()\n\n// NUMA\nmlos_tensor_alloc_numa()\n\n// Batch Scheduling\nmlos_sched_submit_batch()\n```\n\n### New APIs in v7.0.0\n\n```c\n// Tensor Fusion\nmlos_fusion_analyze()\nmlos_fusion_execute()\nmlos_fusion_get_stats()\n\n// Deduplication\nmlos_dedup_hash_tensor()\nmlos_dedup_enable()\nmlos_dedup_analyze_models()\nmlos_dedup_get_stats()\n```\n\n---\n\n## Recommendations\n\n### For Single-Model Deployments\n\n1. **Enable tensor fusion** at model load time\n2. **Use tensor pools** for high-throughput inference\n3. **Set NUMA affinity** on multi-socket systems\n\n### For Multi-Model Deployments\n\n1. **Enable both fusion and deduplication**\n2. **Group similar models** for better dedup hit rates\n3. **Monitor memory savings** via stats APIs\n\n### For Batch Inference\n\n1. **Use `mlos_sched_submit_batch()`** for multi-task submissions\n2. **Tune batch sizes** based on workload\n3. **Pre-warm tensor pools** before high-load periods\n\n---\n\n## What's Next\n\nThe v7.0.0 release establishes a strong foundation for future optimizations:\n\n- **GPU memory pooling** (planned for v7.1.0)\n- **Dynamic tensor fusion** for runtime graph changes\n- **Distributed deduplication** across nodes\n- **Profile-guided optimization** based on runtime metrics\n\n---\n\n## Conclusion\n\nThe journey from v6.3.0-alpha to v7.0.0 represents a significant advancement in MLOS Core's performance capabilities. Through careful optimization of the runtime scheduler and innovative memory management techniques, we've achieved:\n\n- **4× reduction** in per-inference overhead\n- **Up to 40%** memory savings for multi-model deployments\n- **Comprehensive test coverage** across 32 validated models\n\nThese improvements make MLOS Core an even more compelling choice for production ML deployments where performance and resource efficiency are critical.\n\n---\n\n*For detailed technical documentation, see:*\n- [Kernel Performance Analysis](https://github.com/mlOS-foundation/core/blob/main/docs/KERNEL_PERFORMANCE_ANALYSIS.md)\n- [Performance Execution Plan](https://github.com/mlOS-foundation/core/blob/main/docs/KERNEL_PERFORMANCE_EXECUTION_PLAN.md)\n- [Changelog](https://github.com/mlOS-foundation/core/blob/main/CHANGELOG.md)",
      "performance_data": {
        "releases_compared": [
          {
            "version": "v6.3.0-alpha",
            "date": "2025-12-31",
            "name": "Binary Distribution Release",
            "type": "alpha"
          },
          {
            "version": "v7.0.0-beta",
            "date": "2026-01-02",
            "name": "First Beta Release",
            "type": "beta"
          },
          {
            "version": "v7.0.0",
            "date": "2026-01-03",
            "name": "Kernel Optimization Release",
            "type": "stable"
          }
        ],
        "metrics": [
          {
            "name": "Per-inference overhead",
            "unit": "μs",
            "values": {
              "v6.3.0-alpha": "~200",
              "v7.0.0-beta": "<50",
              "v7.0.0": "<50"
            },
            "improvement": "4× reduction"
          },
          {
            "name": "Tensor pool throughput",
            "unit": "ops/sec",
            "values": {
              "v6.3.0-alpha": "N/A",
              "v7.0.0-beta": "3.6M",
              "v7.0.0": "3.6M"
            },
            "improvement": "New capability"
          },
          {
            "name": "Lock acquisitions (batch=4)",
            "unit": "locks",
            "values": {
              "v6.3.0-alpha": 4,
              "v7.0.0-beta": 1,
              "v7.0.0": 1
            },
            "improvement": "4× reduction"
          },
          {
            "name": "Tensor fusion savings",
            "unit": "%",
            "values": {
              "v6.3.0-alpha": 0,
              "v7.0.0-beta": 0,
              "v7.0.0": "15-25"
            },
            "improvement": "15-25% memory reduction"
          },
          {
            "name": "Deduplication savings",
            "unit": "%",
            "values": {
              "v6.3.0-alpha": 0,
              "v7.0.0-beta": 0,
              "v7.0.0": "10-20"
            },
            "improvement": "10-20% memory reduction"
          },
          {
            "name": "Combined memory savings",
            "unit": "%",
            "values": {
              "v6.3.0-alpha": 0,
              "v7.0.0-beta": 0,
              "v7.0.0": "25-40"
            },
            "improvement": "25-40% memory reduction"
          },
          {
            "name": "Kernel tests",
            "unit": "tests",
            "values": {
              "v6.3.0-alpha": 8,
              "v7.0.0-beta": 12,
              "v7.0.0": 18
            },
            "improvement": "125% increase"
          },
          {
            "name": "Models with golden data",
            "unit": "models",
            "values": {
              "v6.3.0-alpha": 18,
              "v7.0.0-beta": 18,
              "v7.0.0": 32
            },
            "improvement": "78% increase"
          }
        ]
      },
      "key_highlights": [
        {
          "title": "4× Faster Inference Overhead",
          "description": "Per-inference overhead reduced from ~200μs to <50μs through tensor pool pre-allocation",
          "icon": "zap"
        },
        {
          "title": "25-40% Memory Reduction",
          "description": "Combined tensor fusion and cross-model deduplication for significant memory savings",
          "icon": "memory"
        },
        {
          "title": "3.6M ops/sec Throughput",
          "description": "Lock-free tensor pool operations enable massive throughput for high-frequency inference",
          "icon": "trending-up"
        },
        {
          "title": "32 Validated Models",
          "description": "Comprehensive golden test data coverage across NLP, embedding, and vision models",
          "icon": "check-circle"
        }
      ],
      "links": [
        {
          "text": "Read Full Post",
          "url": "https://mlosfoundation.org/blog.html?id=kernel-performance-evolution-v7",
          "primary": true
        },
        {
          "text": "v7.0.0 Release",
          "url": "https://github.com/mlOS-foundation/core/releases/tag/v7.0.0",
          "primary": false
        },
        {
          "text": "Performance Analysis Docs",
          "url": "https://github.com/mlOS-foundation/core/blob/main/docs/KERNEL_PERFORMANCE_ANALYSIS.md",
          "primary": false
        },
        {
          "text": "Changelog",
          "url": "https://github.com/mlOS-foundation/core/blob/main/CHANGELOG.md",
          "primary": false
        },
        {
          "text": "E2E Test Reports",
          "url": "https://mlos-foundation.github.io/system-test/",
          "primary": false
        }
      ],
      "tags": [
        "performance",
        "kernel",
        "memory-optimization",
        "tensor-fusion",
        "deduplication",
        "benchmarks",
        "release-analysis"
      ],
      "related_releases": [
        "v6.3.0-alpha",
        "v7.0.0-beta",
        "v7.0.0"
      ],
      "source_repo": "mlOS-foundation/core-releases"
    }
  ]
}